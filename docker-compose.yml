# docker-compose.yml — Local testing for Fractal Agents Runtime
#
# Usage:
#   1. Build the images (from repo root):
#        docker compose build python-runtime
#        docker compose build ts-runtime
#
#   2. Copy .env.example to .env and fill in required values:
#        cp .env.example .env
#
#   3. Start the runtimes:
#        docker compose up python-runtime          # Python only
#        docker compose up ts-runtime              # TypeScript only
#        docker compose up python-runtime ts-runtime  # Both
#
#   4. Test endpoints:
#        # Python runtime (port 9091)
#        curl http://localhost:9091/health
#        curl http://localhost:9091/info
#        curl http://localhost:9091/openapi.json
#
#        # TypeScript runtime (port 9092)
#        curl http://localhost:9092/health
#        curl http://localhost:9092/info
#
# Requires:
#   - Supabase running on supabase_network_immoflow-platform
#   - Langfuse stack running (docker compose -f docker-compose.langfuse.yml up -d)
#     OR create the network manually: docker network create langfuse_network
#   - .env file with at minimum: OPENAI_API_KEY (or ANTHROPIC_API_KEY)
#
# The embeddings, rerankings, and ministral services are optional and
# disabled by default (replicas: 0). Enable them as needed.

volumes:
  hf_cache:

networks:
  supabase:
    external: true
    name: supabase_network_immoflow-platform
  langfuse:
    external: true
    name: langfuse_network

services:
  # ===========================================================================
  # Fractal Agents Runtime — Python/Robyn (internal 8081, external 9091)
  # ===========================================================================
  python-runtime:
    build:
      context: .
      dockerfile: .devops/docker/python.Dockerfile
    image: fractal-agents-runtime-python:local-dev
    ulimits:
      nproc:
        soft: 65535
        hard: 65535
    ports:
      - "9091:8081"
    env_file: .env
    environment:
      - ROBYN_HOST=0.0.0.0
      - ROBYN_PORT=8081
      - ROBYN_WORKERS=1
    networks:
      - default
      - supabase
      - langfuse
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-4", "-f", "http://localhost:8081/health"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 10s

  # ===========================================================================
  # Fractal Agents Runtime — TypeScript/Bun (internal 3000, external 9092)
  # ===========================================================================
  ts-runtime:
    build:
      context: .
      dockerfile: .devops/docker/ts.Dockerfile
    image: fractal-agents-runtime-ts:local-dev
    ports:
      - "9092:3000"
    env_file: .env
    environment:
      - PORT=3000
    networks:
      - default
      - supabase
      - langfuse
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "bun",
          "-e",
          "fetch('http://localhost:3000/health').then(r => r.ok ? process.exit(0) : process.exit(1)).catch(() => process.exit(1))",
        ]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 10s

  # ===========================================================================
  # Langfuse v3 — see docker-compose.langfuse.yml (separate stack)
  # ===========================================================================
  #
  # Start Langfuse independently, then runtimes reach it via langfuse_network:
  #   docker compose -f docker-compose.langfuse.yml up -d
  #   UI: http://localhost:3003  |  Login: admin@fractal.local / FractalAdmin123!
  #
  # Runtimes resolve langfuse-web:3000 over the shared langfuse_network.
  # Set LANGFUSE_BASE_URL=http://langfuse-web:3000 in runtime env files.

  # ===========================================================================
  # Mock LLM — fake OpenAI API for pure runtime overhead benchmarks
  # ===========================================================================
  #
  # Disabled by default (replicas: 0). Enable for benchmarking:
  #   docker compose up mock-llm --scale mock-llm=1
  #
  # Then run k6 with:
  #   MODEL_NAME=openai:mock-gpt-4o OPENAI_BASE_URL=http://mock-llm:11434/v1 k6 run ...
  #
  # Env tunables (see benchmarks/mock-llm/server.ts):
  #   MOCK_LLM_DELAY_MS=10        Base delay before first response
  #   MOCK_LLM_STREAM_DELAY_MS=5  Delay between SSE chunks
  #   MOCK_LLM_MODEL=mock-gpt-4o  Model name echoed in responses

  mock-llm:
    deploy:
      replicas: 0
    image: oven/bun:latest
    working_dir: /app
    volumes:
      - ./benchmarks/mock-llm:/app:ro
    environment:
      - MOCK_LLM_PORT=11434
      - MOCK_LLM_DELAY_MS=10
      - MOCK_LLM_STREAM_DELAY_MS=5
      - MOCK_LLM_MODEL=mock-gpt-4o
    command: ["bun", "run", "server.ts"]
    restart: no
    healthcheck:
      test:
        [
          "CMD",
          "bun",
          "-e",
          "fetch('http://localhost:11434/health').then(r => r.ok ? process.exit(0) : process.exit(1)).catch(() => process.exit(1))",
        ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s

  # ===========================================================================
  # Optional infrastructure services (disabled by default)
  # ===========================================================================

  embeddings:
    deploy:
      replicas: 0
    image: ghcr.io/huggingface/text-embeddings-inference:86-1.6
    volumes:
      - hf_cache:/.hf_cache
    ports:
      - "8011-8014:8080"
    ipc: host
    # Replace runtime with devices for CDI
    devices:
      - nvidia.com/gpu=all
    device_cgroup_rules:
      - "c 195:* rmw"
      - "c 236:* rmw"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - USE_FLASH_ATTENTION=True
      - HF_HUB_ENABLE_HF_TRANSFER=1
      - HF_HOME=/.hf_cache
      - RUST_LOG=info
      # Performance tuning for embedding models
      - OMP_NUM_THREADS=8
      - MKL_NUM_THREADS=8
      - TOKENIZERS_PARALLELISM=true
    restart: no
    env_file: .env
    command:
      [
        "--model-id",
        "jinaai/jina-embeddings-v2-base-de",
        "--hostname",
        "0.0.0.0",
        "--port",
        "8080",
        "--huggingface-hub-cache",
        "/.hf_cache",
        "--tokenization-workers",
        "16",
        "--max-concurrent-requests",
        "1024",
        "--max-batch-tokens",
        "65536",
        "--max-batch-requests",
        "256",
        "--max-client-batch-size",
        "128",
        "--auto-truncate",
        "--payload-limit",
        "10000000",
      ]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: json-file
      options:
        max-size: "100m"
        max-file: "1"

  rerankings:
    deploy:
      replicas: 0
    image: ghcr.io/huggingface/text-embeddings-inference:86-1.6
    volumes:
      - hf_cache:/.hf_cache
    ports:
      - 8020:8080
    container_name: rerankings
    env_file: .env
    ipc: host
    # Replace runtime with devices for CDI
    devices:
      - nvidia.com/gpu=all
    device_cgroup_rules:
      - "c 195:* rmw"
      - "c 236:* rmw"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - USE_FLASH_ATTENTION=True
      - HF_HUB_ENABLE_HF_TRANSFER=1
      - HF_HOME=/.hf_cache
      - RUST_LOG=info
      # Performance tuning for reranking models
      - OMP_NUM_THREADS=8
      - MKL_NUM_THREADS=8
      - TOKENIZERS_PARALLELISM=true
    restart: no
    command:
      [
        "--model-id",
        "BAAI/bge-reranker-v2-m3",
        "--hostname",
        "0.0.0.0",
        "--port",
        "8080",
        "--huggingface-hub-cache",
        "/.hf_cache",
        "--tokenization-workers",
        "16",
        "--max-concurrent-requests",
        "1024",
        "--max-batch-tokens",
        "32768",
        "--max-batch-requests",
        "256",
        "--max-client-batch-size",
        "64",
        "--auto-truncate",
        "--payload-limit",
        "4000000",
      ]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging:
      driver: json-file
      options:
        max-size: "100m"
        max-file: "1"

  ministral:
    ulimits:
      nofile:
        soft: 8192
        hard: 16384
    pids_limit: 512
    device_cgroup_rules:
      - "c 195:* rmw"
      - "c 236:* rmw"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HUGGING_FACE_HUB_TOKEN=${HUGGINGFACEHUB_API_TOKEN:-your_hf_api_access_token}
      - HF_HOME=/.hf_cache
      - VLLM_ATTENTION_BACKEND=FLASHINFER
      - HF_HUB_ENABLE_HF_TRANSFER=1
      - MAX_PARALLEL_LOADING_WORKERS=2
    shm_size: "2gb"
    deploy:
      replicas: 1
    volumes:
      - hf_cache:/.hf_cache
    restart: no
    image: vllm/vllm-openai:cu130-nightly
    ports:
      - 7374:80
    devices:
      - nvidia.com/gpu=all
    command:
      [
        "mistralai/Ministral-3-3B-Instruct-2512",
        "--host",
        "0.0.0.0",
        "--port",
        "80",
        "--served-model-name",
        "ministral-3b-instruct",
        "--max-model-len",
        "8192",
        "--max-num-batched-tokens",
        "8192",
        "--kv-cache-dtype",
        "fp8",
        "--enable-auto-tool-choice",
        "--tool-call-parser",
        "mistral",
        "--gpu-memory-utilization",
        "0.875",
        "--seed",
        "4272",
        "--max-num-seqs",
        "9",
        "--trust-remote-code",
      ]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://ministral:80/health"]
      interval: 30s
      timeout: 5s
      retries: 5
